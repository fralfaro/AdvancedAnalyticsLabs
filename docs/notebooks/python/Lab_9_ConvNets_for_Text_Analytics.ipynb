{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fralfaro/AdvancedAnalyticsLabs/blob/master/docs/notebooks/python/Lab_9_ConvNets_for_Text_Analytics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iDWxyBFpRaP-"
      },
      "source": [
        "# Convolutional Neural Networks for Text Analytics\n",
        "\n",
        "In this lab, we will work on convolutional networks applied to our IMDB embedding. We will test two architectures: A simple convnet using a sequence of layers, and a more complex one implementing a paper and using [Keras' Model API](https://keras.io/models/model/).\n",
        "\n",
        "First, let's import the data as we had it last week. I saved the output using numpy's [savetxt](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.savetxt.html) and will load it with [genfromtxt](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.genfromtxt.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "qJm4jZARRavt"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "\n",
        "# Keras\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.utils import get_file\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import categorical_accuracy\n",
        "from tensorflow.keras.preprocessing import image, sequence\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "wqiEF7QlRjcd"
      },
      "outputs": [],
      "source": [
        "# Others\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# For AUC estimation and ROC plots\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Plots\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "5MQLA_fBbRxh"
      },
      "outputs": [],
      "source": [
        "# Get the data and unzip it.\n",
        "!gdown 'https://drive.google.com/uc?export=download&id=1-zwO4umZ2g0eSG5IciYCFSV-fHqP1O35'\n",
        "\n",
        "# Extract the files.\n",
        "!unzip IMDB_Preprocessed.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "2GPXA6BcRl2c"
      },
      "outputs": [],
      "source": [
        "# Read word embeddings\n",
        "embedding_matrix = np.genfromtxt('IMDB_Preprocessed/IMDB_EmbeddingMatrix.txt')\n",
        "\n",
        "# Read the padded text sequences. Uses numpy function that loads from a previously saved txt.\n",
        "data = np.genfromtxt('IMDB_Preprocessed/IMDB_Padded.txt')\n",
        "labels = np.genfromtxt('IMDB_Preprocessed/IMDB_Labels.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rQPF1VxNR6Vi"
      },
      "source": [
        "We will also generate a train and test set to make a fair comparison between our models. From here onwards we will use the [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function from scikit-learn. This works for any general dataset, not only scorecards.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "S3CDwHxnR6ts"
      },
      "outputs": [],
      "source": [
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.33,\n",
        "                                                    random_state=20190327, \n",
        "                                                    stratify = labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bQLWCpotc8Dp"
      },
      "source": [
        "From last week, we could build a CNN. We now will create a much more powerful structure, a convolutional neural network with parallel convolutional layers following [Kim et al. (2015)](https://arxiv.org/abs/1408.5882), to create a much better model. First let's create a sequential model for comparison.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## Sequential ConvNet \n",
        "\n",
        "We will try a very simple architecture:\n",
        "\n",
        "1. A 1D convolution of size 3 (so three word sequences)\n",
        "2. A second 1D convolution of size 3 (so more complex word sequences)\n",
        "3. Dropout to avoid overfitting\n",
        "4. Max pooling to get the most important sequences.\n",
        "5. A flattening layer.\n",
        "6. A dense layer of 128 neurons.\n",
        "7. Dropout with p = 0.5\n",
        "8. A softmax output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "UXLbTIX1jO4c"
      },
      "outputs": [],
      "source": [
        "# Final model.\n",
        "model = Sequential()\n",
        "embedding_layer = Embedding(len(embedding_matrix),           # Words in the embedding.\n",
        "                            300,                           # Embedding dimension\n",
        "                            weights=[embedding_matrix],    # The weights we just calculated\n",
        "                            input_length=600,              # The maximum number of words.\n",
        "                            trainable=False)               # To NOT recalculate weights!\n",
        "\n",
        "model.add(embedding_layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hQk6zaIkjX4L"
      },
      "source": [
        "Now we add our first convolution. The operation is to add a Convolution1D layer, called [Conv1D](https://keras.io/layers/convolutional/) as we did last week, over the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "TuAbKsgPjXWa"
      },
      "outputs": [],
      "source": [
        "model.add(Conv1D(filters=128,           # How many filters to calculate.\n",
        "                 kernel_size=3,         # How many words to calculate per filter.\n",
        "                 padding='valid',       # Add padding?\n",
        "                 activation='relu',     # What activation to use?\n",
        "                 strides=1)             # What stride to use?\n",
        "         )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b7VtMxnpjeT4"
      },
      "source": [
        "(if you get a warning, ignore it, it is a slight difference between Keras and Tensorflow).\n",
        "\n",
        "We now add our second convolutional layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "90imlPz5jcAQ"
      },
      "outputs": [],
      "source": [
        "model.add(Conv1D(filters=128,           # How many filters to calculate.\n",
        "                 kernel_size=3,         # How many words to calculate per filter.\n",
        "                 padding='valid',       # Add padding?\n",
        "                 activation='relu',     # What activation to use?\n",
        "                 strides=1)             # What stride to use?\n",
        "         )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LlQrFdpijkfK"
      },
      "source": [
        "And now we add max pooling. We will reduce filters by a factor of 3, to keep with the number. Basically, we will only choose one out of every three sequences that we are studying, the most intense one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "DXtaoNJFjixX"
      },
      "outputs": [],
      "source": [
        "model.add(MaxPooling1D(pool_size = 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pWsiC45yjpqZ"
      },
      "source": [
        "Now we are almost ready. First we flatten, and then add a dense layer. Dropout and an output layer finalize the problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "biilkk6NjmS-"
      },
      "outputs": [],
      "source": [
        "# Flatten\n",
        "model.add(Flatten())\n",
        "\n",
        "# Dense Layer of size 128 with Dropout\n",
        "model.add(Dense(128, activation = 'relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Output layer of size 1\n",
        "model.add(Dense(1, activation = 'sigmoid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dReDpjG3juMd"
      },
      "source": [
        "That's it! Now we can check the summary of the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "BGAYGITgjrwU"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x2naM3CKjyz8"
      },
      "source": [
        "As last time, we need a binary_crossentropy error function. I will use Adam for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "pBjenWSJjv70"
      },
      "outputs": [],
      "source": [
        "# Use Adam as optimizer, with a binary_crossentropy error.\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['acc'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pX1YS2Hij2-E"
      },
      "source": [
        "Now we train! We will calculate the model over the training set, and then estimate the AUC of the model over both the training and testing set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "amLfhcB_j0v-"
      },
      "outputs": [],
      "source": [
        "# Fit the model\n",
        "model.fit(X_train, y_train, validation_split=0.33, epochs=20, batch_size=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MzOWS0fSj7-i"
      },
      "source": [
        "The model reaches 100% predictive capability very quickly in the train set, but the validation set overadjust quite quickly too! You should play around with your architectures until you are happy with one that represents correctly the variability of the data.\n",
        "\n",
        "The following code shows the training history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ZukxLx3kj6Pt"
      },
      "outputs": [],
      "source": [
        "# Plot history\n",
        "\n",
        "print(model.history.history.keys())\n",
        "\n",
        "sns.set_style(\"darkgrid\")\n",
        "plt.plot(model.history.history['loss'])\n",
        "plt.title('Training Loss')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "sns.set_style(\"darkgrid\")\n",
        "plt.plot(model.history.history['val_loss'])\n",
        "plt.title('Validation Loss')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4k8gN3C_kJhI"
      },
      "source": [
        "We should have probably stopped training at around 6 epochs or so. It's possible to do this automatically, you need to add a [callback](https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/). Read the link for a tutorial. You must either do it manually or automatically, but you **must** do it.\n",
        "\n",
        "We can now calculate the ROC curve of the model with the following code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "8o7SP0H4j-uz"
      },
      "outputs": [],
      "source": [
        "# Calculate outputs in test set\n",
        "prob_test = model.predict(X_test, verbose = 1)\n",
        "prob_train = model.predict(X_train, verbose = 1)\n",
        "\n",
        "# Compute ROC curve and ROC area\n",
        "fpr, tpr, _ = roc_curve(y_train, prob_train)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "print('\\nAUC train: ', roc_auc)\n",
        "\n",
        "# Compute ROC curve and ROC area\n",
        "fpr, tpr, _ = roc_curve(y_test, prob_test)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "print('AUC test: ', roc_auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z4kL2K8DkNa9"
      },
      "source": [
        "Not a bad model at all! This model can actually be improved much further, by carefully designing an architecture that exploits the structure of the words and the complexity of the dataset.\n",
        "\n",
        "Let's finally plot the ROC curve over the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "5YCoHUb-kLI4"
      },
      "outputs": [],
      "source": [
        "sns.set('talk', 'darkgrid', 'dark', font_scale=1, \\\n",
        "        rc={\"lines.linewidth\": 2, 'grid.linestyle': '--'})\n",
        "\n",
        "lw = 2\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange',\n",
        "         lw=lw, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "\n",
        "# I am saving the output as a PDF for easy exporting.\n",
        "plt.savefig('roc_auc.pdf', format = \"pdf\")\n",
        "\n",
        "# Now I show the plot inline.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OmrpgPBEkYCW"
      },
      "source": [
        "## Keras Model API\n",
        "\n",
        "As we saw in the lectures, Google's LeNet showed that stacking parallel layers of convolution (so, not sequential models) seemed like a good idea. This principle was captured by [Kim et al. (2015)](https://arxiv.org/abs/1408.5882) to create an architecture using multiple filters of different lengths, before a final dense layer. There are no stacked convolutions here.\n",
        "\n",
        "Which one performs better? Let's see. The plot of this architecture follows.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1W41RmVNWLPHSSm0gR9yEzVPBpyZJ3ym7)\n",
        "\n",
        "Having parallel layers is the opossite of having a sequential model, so we need to use the excellent [Model API](https://keras.io/models/model/) of Keras, which will allow us to get as creative as we want with our models.\n",
        "\n",
        "Our architecture now is as follows:\n",
        "\n",
        "1. An embedding layer, just like before.\n",
        "2. A parallel model of four layers, each consisting of:\n",
        "    - A Conv1D layer with kernel sizes [2, 3, 4, 6] (arbitrary, play around with this), with 128 filters (also arbitrary).\n",
        "    - A max pooling layer of size Embedding Dimension - Filter Size + 1. Basically get just the best feature per filter.\n",
        "    - A flattening layer to turn this into a vector.\n",
        "3. A dense layer of size 128.\n",
        "4. Dropout with probability 0.5.\n",
        "5. An output layer of size 1.\n",
        "\n",
        "The authors discuss that this model will have a much better capacity to study complex structures without overfitting. One critique of the model is that you are only looking at n-grams without focusing on complex combined structures.\n",
        "\n",
        "So, let's start by creating the stacked structure. Given that these go in parallel, we need to create the structure Conv1D - MaxPool - Flattening, and then append them into an object. Finally, the special [Concatenate](https://keras.io/layers/merge/#concatenate) layer is the one that puts them in parallel next to each other.\n",
        "\n",
        "The following codes does this at once for all filter sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "1lOMnJWxkRbO"
      },
      "outputs": [],
      "source": [
        "# Filter sizes to use.\n",
        "filter_sizes = (2,3,4,6)\n",
        "\n",
        "# Initialize. We need to give it the input dimension (from the Embedding!)\n",
        "graph_in = Input(shape=(600, 300))\n",
        "convs = []\n",
        "avgs = []\n",
        "\n",
        "# This for stacks the layers. Inside each for, we build the sequence of layer. The command \"append\" adds\n",
        "# that to the \"conv\" variable, which is simply a stack of convolutions.\n",
        "for fsz in filter_sizes:\n",
        "    conv = Conv1D(filters=128,\n",
        "                  kernel_size=fsz,\n",
        "                         padding='valid',\n",
        "                         activation='relu',\n",
        "                         strides=1)(graph_in) # Note the (graph_in). This means \"put this layer AFTER the graph_in layer.\n",
        "    pool = MaxPooling1D(pool_size=600 - fsz + 1)(conv) # Put this layer AFTER the convolution just created.\n",
        "    flattenMax = Flatten()(pool) # Flatten the pooling layer.\n",
        "    convs.append(flattenMax) # Append this to the convs object that saves the stack.\n",
        "    \n",
        "# Concatenate layers.\n",
        "if len(filter_sizes)>1:\n",
        "    out = Concatenate()(convs)\n",
        "else:\n",
        "    out = convs[0]\n",
        "\n",
        "graph = Model(inputs=graph_in, outputs=out, name=\"graphModel\")\n",
        "\n",
        "graph.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "oItUSWfjE9b4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from IPython.display import Image\n",
        "%matplotlib inline\n",
        "\n",
        "plot_model(graph, show_shapes=True, show_layer_names=True, to_file='GraphModel.png')\n",
        "Image(retina=True, filename='GraphModel.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IYw1QM12lZ9Y"
      },
      "source": [
        "Note that we named the model \"graphModel\", and that it follows the architecture we discussed. This is just the \"middle\" part of the network, so we need to add an input layer, and an output layer.\n",
        "\n",
        "We can do this with a traditional sequential model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "nqsfdGz4lVpp"
      },
      "outputs": [],
      "source": [
        "# Final model\n",
        "model = Sequential()\n",
        "embedding_layer = Embedding(len(embedding_matrix),           # Words in the embedding.\n",
        "                            300,                           # Embedding dimension\n",
        "                            weights=[embedding_matrix],    # The weights we just calculated\n",
        "                            input_length=600,              # The maximum number of words.\n",
        "                            trainable=False)               # To NOT recalculate weights!\n",
        "\n",
        "model.add(embedding_layer)\n",
        "\n",
        "# Now we add our graph model\n",
        "model.add(graph)\n",
        "\n",
        "# Add a few layers\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# adam = Adam(clipnorm=.1)\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "AgUeYhLBloM1"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "W8KcjxfEFeP9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from IPython.display import Image\n",
        "%matplotlib inline\n",
        "\n",
        "plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')\n",
        "Image(retina=True, filename='model.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f7Firvd_lwqe"
      },
      "source": [
        "The summary shows what we expected: The graph model is thought to be just one more layer of our main sequential model. This allow us to train very complex models! An example would be Google's LeNet (see example [here](https://colab.research.google.com/drive/1CVm50PGE4vhtB5I_a_yc4h5F-itKOVL9)). The goal is simply to build the basic building blocks independently and stack those.\n",
        "\n",
        "Let's train our model. Now the code is the same as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "IUt5_nBhltml"
      },
      "outputs": [],
      "source": [
        "# Fit the model\n",
        "model.fit(X_train, y_train, validation_split=0.33, epochs=20, batch_size=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "EGUnus8OmQr2"
      },
      "outputs": [],
      "source": [
        "# Plot history\n",
        "sns.set_style(\"darkgrid\")\n",
        "plt.plot(model.history.history['loss'])\n",
        "plt.title('Training Loss')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "sns.set_style(\"darkgrid\")\n",
        "plt.plot(model.history.history['val_loss'])\n",
        "plt.title('Validation Loss')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ttea9vV_muYp"
      },
      "source": [
        "Would you stop training? If so, where?\n",
        "\n",
        "Let's plot the ROC curve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "0V_RCZj-mT7-"
      },
      "outputs": [],
      "source": [
        "# Calculate outputs in test set\n",
        "prob_test = model.predict(X_test, verbose = 1)\n",
        "prob_train = model.predict(X_train, verbose = 1)\n",
        "\n",
        "# Compute ROC curve and ROC area\n",
        "fpr, tpr, _ = roc_curve(y_train, prob_train)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "print('\\nAUC train: ', roc_auc)\n",
        "\n",
        "# Compute ROC curve and ROC area\n",
        "fpr, tpr, _ = roc_curve(y_test, prob_test)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "print('AUC test: ', roc_auc)\n",
        "\n",
        "sns.set('talk', 'darkgrid', 'dark', font_scale=1, \\\n",
        "        rc={\"lines.linewidth\": 2, 'grid.linestyle': '--'})\n",
        "\n",
        "lw = 2\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange',\n",
        "         lw=lw, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig('roc_auc.pdf', format = \"pdf\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F2PncegAm6If"
      },
      "source": [
        "Now we are getting impressive results! This is the power of a well trained neural network. This is already reaching close to state-of-the-art accuracies.\n",
        "\n",
        "## Self-study\n",
        "\n",
        "The state of the art methods can reach around 0.9-0.95 AUC. Can you improve upon these results?\n",
        "\n",
        "You have everything you need to solve all the practical parts of the coursework 2. Experiment away!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Lab 10 - ConvNets for Text Analytics.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
